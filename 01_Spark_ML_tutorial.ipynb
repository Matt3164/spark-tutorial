{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib: Basic Statistics and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data and creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective is to play with a small dataset from the Hackathon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "## Currently, do not run any command. \n",
    "\n",
    "Re-run it at the end if you have the time. For now, let's just see how to create a dataframe.\n",
    "\n",
    "First, install the h5py if you don't have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py\n",
      "  Downloading h5py-2.7.1-cp27-cp27mu-manylinux1_x86_64.whl (4.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.8MB 258kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/lib/python2.7/dist-packages (from h5py)\n",
      "Requirement already satisfied: six in /usr/lib/python2.7/dist-packages (from h5py)\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(\"eightieth.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = f[\"S2\"], f[\"TOP_LANDCOVER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract images from np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ims = list()\n",
    "lbls = list()\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    im=X[i,::]\n",
    "    lbl=Y[i]\n",
    "    \n",
    "    ims.append(im)\n",
    "    lbls.append(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(zip(ims, lbls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute some simple features from an image. Returns list of features and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import Row\n",
    "\n",
    "def compute_features(t):\n",
    "    im, lbl = t\n",
    "    \n",
    "    means = im.mean(axis=(0, 1)).astype(float).tolist()\n",
    "    \n",
    "    stds = im.std(axis=(0, 1)).astype(float).tolist()\n",
    "    \n",
    "    return means+stds+lbl.astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[620.60546875,\n",
       " 859.84375,\n",
       " 928.0703125,\n",
       " 2435.7265625,\n",
       " 245.32203674316406,\n",
       " 122.14816284179688,\n",
       " 110.50434875488281,\n",
       " 184.1570281982422,\n",
       " 5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_features((im, lbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform it to a dataframe. Requires giving a name to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_names = [\"Mean_{}\".format(i) for i in range(4)]\n",
    "    \n",
    "std_names = [\"Std_{}\".format(i) for i in range(4)]\n",
    "\n",
    "df = data.map(compute_features).toDF(mean_names+std_names+[\"Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to the Google Storage (HDFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.write.parquet(\"gs://mlg-store/isae/im-features-all.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The hands on starts here\n",
    "\n",
    "Now you can execute commands.\n",
    "\n",
    "# Processing\n",
    "A dataset is already available to use: \n",
    "\n",
    "Links:\n",
    "* https://storage.googleapis.com/mlg-store/isae/im-features-100000.parquet/_SUCCESS\n",
    "* https://storage.googleapis.com/mlg-store/isae/im-features-100000.parquet/part-00000-6685d2e1-601f-490f-be13-586b1b0b260d-c000.snappy.parquet\n",
    "* https://storage.googleapis.com/mlg-store/isae/im-features-100000.parquet/part-00001-6685d2e1-601f-490f-be13-586b1b0b260d-c000.snappy.parquet\n",
    "\n",
    "Please respect the order, Root folder:\n",
    "* ./im-features-100000.parquet/\n",
    "    * _SUCCESS\n",
    "    * part-00000-6685d2e1-601f-490f-be13-586b1b0b260d-c000.snappy.parquet\n",
    "    * part-00001-6685d2e1-601f-490f-be13-586b1b0b260d-c000.snappy.parquet\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresDF=None\n",
    "featuresDF = sqlContext.read.parquet(\"./im-features-100000.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, parsing the file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Mean_0: double (nullable = true)\n",
      " |-- Mean_1: double (nullable = true)\n",
      " |-- Mean_2: double (nullable = true)\n",
      " |-- Mean_3: double (nullable = true)\n",
      " |-- Std_0: double (nullable = true)\n",
      " |-- Std_1: double (nullable = true)\n",
      " |-- Std_2: double (nullable = true)\n",
      " |-- Std_3: double (nullable = true)\n",
      " |-- Label: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = featuresDF.describe().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Statistics count\n",
      "summary: count\n",
      "Std_3: 100\n",
      "Std_2: 100\n",
      "Std_1: 100\n",
      "Std_0: 100\n",
      "Label: 100\n",
      "Mean_0: 100\n",
      "Mean_1: 100\n",
      "Mean_2: 100\n",
      "Mean_3: 100\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics mean\n",
      "summary: mean\n",
      "Std_3: 184.23522521972657\n",
      "Std_2: 77.61876323699951\n",
      "Std_1: 92.73744741439819\n",
      "Std_0: 167.9272707939148\n",
      "Label: 6.26\n",
      "Mean_0: 853.62453125\n",
      "Mean_1: 994.3158203125\n",
      "Mean_2: 1030.4110546875\n",
      "Mean_3: 2450.00140625\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics stddev\n",
      "summary: stddev\n",
      "Std_3: 95.2802802807138\n",
      "Std_2: 42.73152720352641\n",
      "Std_1: 51.76375618880337\n",
      "Std_0: 94.95745800620809\n",
      "Label: 3.737930070523151\n",
      "Mean_0: 244.62828105437603\n",
      "Mean_1: 137.94634200835668\n",
      "Mean_2: 104.88589335373015\n",
      "Mean_3: 221.83929702320293\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics min\n",
      "summary: min\n",
      "Std_3: 61.98518371582031\n",
      "Std_2: 17.851411819458008\n",
      "Std_1: 25.63669776916504\n",
      "Std_0: 30.777761459350586\n",
      "Label: 2\n",
      "Mean_0: 492.98046875\n",
      "Mean_1: 788.02734375\n",
      "Mean_2: 871.04296875\n",
      "Mean_3: 2075.984375\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics max\n",
      "summary: max\n",
      "Std_3: 722.493896484375\n",
      "Std_2: 206.45277404785156\n",
      "Std_1: 254.65744018554688\n",
      "Std_0: 442.2795104980469\n",
      "Label: 12\n",
      "Mean_0: 1621.6171875\n",
      "Mean_1: 1445.2890625\n",
      "Mean_2: 1341.890625\n",
      "Mean_3: 3299.61328125\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for l in result:\n",
    "    print \"----------------------\"\n",
    "    r = l.asDict()\n",
    "    print \"Statistics {}\".format(r[\"summary\"])\n",
    "    for key in r.keys():\n",
    "        print \"{0}: {1}\".format(key, r[key])\n",
    "    print \"----------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute statistics by labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    5|   41|\n",
      "|    3|   17|\n",
      "|   12|   28|\n",
      "|    2|   11|\n",
      "|    4|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresDF.select(\"label\").groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning with Apache Spark\n",
    "Now that the inputs are defined, we can apply some basics (or advanced) data processing functions to classify the type of interactions (i.e. \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "col_names=featuresDF.columns\n",
    "col_names.remove(\"Label\")\n",
    "s = StringIndexer(inputCol=\"Label\", outputCol=\"idx_label\").fit(featuresDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = s.transform(featuresDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = featuresDF.randomSplit([0.6,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy on both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|prediction|idx_label|count|\n",
      "+----------+---------+-----+\n",
      "|       1.0|      1.0|   10|\n",
      "|       0.0|      0.0|   19|\n",
      "|       0.0|      2.0|    7|\n",
      "|       0.0|      3.0|    3|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test).select(\"prediction\", \"idx_label\").groupBy(\"prediction\", \"idx_label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "preds = model.transform(test)\n",
    "print preds.where(preds.prediction == preds.idx_label).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice:\n",
    "* Compute classic multi classification statistics precision recall etc ... for each label using spark \n",
    "\n",
    "Hint: compute the confusion matrix in spark and the final divisions in local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Try applying a PCA before learning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"pca_features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, pca, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|prediction|idx_label|count|\n",
      "+----------+---------+-----+\n",
      "|       1.0|      1.0|    7|\n",
      "|       0.0|      1.0|    3|\n",
      "|       1.0|      0.0|   14|\n",
      "|       1.0|      2.0|    4|\n",
      "|       0.0|      0.0|    5|\n",
      "|       1.0|      3.0|    3|\n",
      "|       0.0|      2.0|    3|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test).select(\"prediction\", \"idx_label\").groupBy(\"prediction\", \"idx_label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try applying a kmeans to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(k=2, seed=1, featuresCol=\"features\", predictionCol=\"kmeans_pred\")\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "kmeans_assemblor = VectorAssembler(inputCols=col_names+[\"kmeans_pred\"], outputCol=\"kmeans_features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"kmeans_features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, kmeans, kmeans_assemblor, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "s = StringIndexer(inputCol=\"Label\", outputCol=\"idx_label\")\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"pca_features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, pca, rf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(pca.k, [2, 5, 8]) \\\n",
    "    .addGrid(rf.maxDepth, [1, 10]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol=\"Label\"),\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|prediction|idx_label|count|\n",
      "+----------+---------+-----+\n",
      "|       1.0|      1.0|    7|\n",
      "|       0.0|      1.0|    3|\n",
      "|       1.0|      0.0|   14|\n",
      "|       1.0|      2.0|    4|\n",
      "|       0.0|      0.0|    5|\n",
      "|       1.0|      3.0|    3|\n",
      "|       0.0|      2.0|    3|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvModel.transform(test).select(\"prediction\", \"idx_label\").groupBy(\"prediction\", \"idx_label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices:\n",
    "### Detect classification noise using multiple models\n",
    "    - If different models keep giving wrong predictions on the same sample, it may be a labeling mistake\n",
    "    - Estimate labeling mistakes\n",
    "\n",
    "### Implement the Viola Jones strategy using Spark\n",
    "    - Learn a classifier (Decision Tree)\n",
    "    - Reduce to zero the FPR (False alarms) using a threshold on probability\n",
    "    - Keep only the examples where the classifier gives a positive answer\n",
    "    - Repeat n times\n",
    "    \n",
    "### Hackathon training:\n",
    "    - Change the features computed on images to increase performance (see first part of the notebook)\n",
    "        - Only means and stds have been used here\n",
    "    - Be sure to grab the raw dataset before hand [here](https://storage.googleapis.com/mlg-store/isae/eightieth.h5)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
